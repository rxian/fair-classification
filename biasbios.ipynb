{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "import os, urllib.request\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "from projection_simplex_vectorized import projection_simplex\n",
    "import postprocess\n",
    "\n",
    "seed = 33\n",
    "transformers.set_seed(seed)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model_dir = \"models/biasbios_squared_loss\"\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = 32\n",
    "lr = 2e-5\n",
    "warmup_ratio = 0.1\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "split_ratio_for_postprocessing = 0.05\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "noise_fn = lambda shape: rng.laplace(loc=0.0, scale=0.2 / 28, size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load BiasBios dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bio': 'female. She has been working with children in camp, community and school settings for the past 8 years. She believes in the importance of cultivating self-love and awareness in black children at a very young age and is excited to be apart of Black Lives Matter Torontoâ€™s Freedom School!', 'title': 26, 'gender': 0}\n"
     ]
    }
   ],
   "source": [
    "label_names = [\n",
    "    \"accountant\", \"architect\", \"attorney\", \"chiropractor\", \"comedian\",\n",
    "    \"composer\", \"dentist\", \"dietitian\", \"dj\", \"filmmaker\", \"interior_designer\",\n",
    "    \"journalist\", \"model\", \"nurse\", \"painter\", \"paralegal\", \"pastor\",\n",
    "    \"personal_trainer\", \"photographer\", \"physician\", \"poet\", \"professor\",\n",
    "    \"psychologist\", \"rapper\", \"software_engineer\", \"surgeon\", \"teacher\",\n",
    "    \"yoga_teacher\"\n",
    "]\n",
    "n_labels = len(label_names)\n",
    "\n",
    "group_names = [\"female\", \"male\"]\n",
    "n_groups = len(group_names)\n",
    "\n",
    "features = datasets.Features({\n",
    "    \"bio\": datasets.Value(\"string\"),\n",
    "    \"title\": datasets.ClassLabel(names=label_names),\n",
    "    \"gender\": datasets.ClassLabel(names=group_names),\n",
    "})\n",
    "\n",
    "train_path = \"data/biasbios/train.pickle\"\n",
    "test_path = \"data/biasbios/test.pickle\"\n",
    "dev_path = \"data/biasbios/dev.pickle\"\n",
    "if any([not os.path.exists(p) for p in [train_path, test_path, dev_path]]):\n",
    "  os.makedirs(\"data/biasbios\", exist_ok=True)\n",
    "  urllib.request.urlretrieve(\n",
    "      \"https://storage.googleapis.com/ai2i/nullspace/biasbios/train.pickle\",\n",
    "      train_path)\n",
    "  urllib.request.urlretrieve(\n",
    "      \"https://storage.googleapis.com/ai2i/nullspace/biasbios/test.pickle\",\n",
    "      test_path)\n",
    "  urllib.request.urlretrieve(\n",
    "      \"https://storage.googleapis.com/ai2i/nullspace/biasbios/dev.pickle\",\n",
    "      dev_path)\n",
    "\n",
    "raw_dataset = {}\n",
    "for split, path in zip([\"train\", \"test\", \"dev\"],\n",
    "                       [train_path, test_path, dev_path]):\n",
    "  rows = {k: [] for k in features}\n",
    "  with open(path, \"rb\") as pickle_file:\n",
    "    for row in pickle.load(pickle_file):\n",
    "      rows[\"gender\"].append(\"female\" if row[\"g\"] == \"f\" else \"male\")\n",
    "      rows[\"title\"].append(row[\"p\"])\n",
    "      rows[\"bio\"].append(rows[\"gender\"][-1] + \". \" +\n",
    "                         row[\"hard_text_untokenized\"])\n",
    "  raw_dataset[split] = datasets.Dataset.from_dict(rows, features=features)\n",
    "raw_dataset = datasets.DatasetDict(raw_dataset)\n",
    "\n",
    "print(raw_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>dp_gap_linf_max</th>\n",
       "      <th>dp_gap_l1_max</th>\n",
       "      <th>dp_gap_l1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>perfect_postprocessed</th>\n",
       "      <td>0.884376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect_predictor</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08644</td>\n",
       "      <td>0.231247</td>\n",
       "      <td>0.231247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       balanced_accuracy  dp_gap_linf_max  dp_gap_l1_max  \\\n",
       "perfect_postprocessed           0.884376              NaN            NaN   \n",
       "perfect_predictor                    NaN          0.08644       0.231247   \n",
       "\n",
       "                       dp_gap_l1_avg  \n",
       "perfect_postprocessed            NaN  \n",
       "perfect_predictor           0.231247  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Group</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accountant</th>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.016898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architect</th>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.036508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attorney</th>\n",
       "      <td>0.068610</td>\n",
       "      <td>0.095177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chiropractor</th>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.009029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comedian</th>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.010444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>composer</th>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.022156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dentist</th>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.044132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dietitian</th>\n",
       "      <td>0.020258</td>\n",
       "      <td>0.001368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dj</th>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.006029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filmmaker</th>\n",
       "      <td>0.012685</td>\n",
       "      <td>0.022236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interior_designer</th>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalist</th>\n",
       "      <td>0.054217</td>\n",
       "      <td>0.047686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.034124</td>\n",
       "      <td>0.006095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>0.094650</td>\n",
       "      <td>0.008210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>painter</th>\n",
       "      <td>0.019456</td>\n",
       "      <td>0.019842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paralegal</th>\n",
       "      <td>0.008232</td>\n",
       "      <td>0.001268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pastor</th>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal_trainer</th>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.003682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photographer</th>\n",
       "      <td>0.047715</td>\n",
       "      <td>0.073987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physician</th>\n",
       "      <td>0.107517</td>\n",
       "      <td>0.089844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poet</th>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.016894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professor</th>\n",
       "      <td>0.292638</td>\n",
       "      <td>0.306737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychologist</th>\n",
       "      <td>0.062520</td>\n",
       "      <td>0.032699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rapper</th>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software_engineer</th>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.027527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surgeon</th>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.053478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teacher</th>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.030418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga_teacher</th>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.001216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Group                female      male\n",
       "Target                               \n",
       "accountant         0.011428  0.016898\n",
       "architect          0.013168  0.036508\n",
       "attorney           0.068610  0.095177\n",
       "chiropractor       0.003789  0.009029\n",
       "comedian           0.003251  0.010444\n",
       "composer           0.005041  0.022156\n",
       "dentist            0.028297  0.044132\n",
       "dietitian          0.020258  0.001368\n",
       "dj                 0.001159  0.006029\n",
       "filmmaker          0.012685  0.022236\n",
       "interior_designer  0.006496  0.001325\n",
       "journalist         0.054217  0.047686\n",
       "model              0.034124  0.006095\n",
       "nurse              0.094650  0.008210\n",
       "painter            0.019456  0.019842\n",
       "paralegal          0.008232  0.001268\n",
       "pastor             0.003344  0.009100\n",
       "personal_trainer   0.003591  0.003682\n",
       "photographer       0.047715  0.073987\n",
       "physician          0.107517  0.089844\n",
       "poet               0.018896  0.016894\n",
       "professor          0.292638  0.306737\n",
       "psychologist       0.062520  0.032699\n",
       "rapper             0.000747  0.006015\n",
       "software_engineer  0.005980  0.027527\n",
       "surgeon            0.010829  0.053478\n",
       "teacher            0.053640  0.030418\n",
       "yoga_teacher       0.007721  0.001216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Group</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>182102</td>\n",
       "      <td>211321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Group  female    male\n",
       "Count  182102  211321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute and print dataset statistics\n",
    "\n",
    "df = datasets.concatenate_datasets([d for d in raw_dataset.values()])\n",
    "df = pd.DataFrame(np.stack(\n",
    "    [np.array(group_names)[df[\"gender\"]],\n",
    "     np.array(label_names)[df[\"title\"]]],\n",
    "    axis=1),\n",
    "                  columns=[\"Group\", \"Target\"])\n",
    "grouped = df.groupby([\"Target\", \"Group\"]).size().unstack()\n",
    "n_labels = len(grouped.index)\n",
    "n_groups = len(grouped.columns)\n",
    "counts = grouped.sum(axis=0)\n",
    "normalized = np.nan_to_num((grouped.to_numpy() / counts.to_numpy())).T\n",
    "diff = np.abs(normalized[:, None, :] - normalized[None, :, :])\n",
    "postprocessor = postprocess.PostProcessor()\n",
    "postprocessor.fit(\n",
    "    np.concatenate([np.eye(n_labels) for _ in range(n_groups)], axis=0),\n",
    "    np.repeat(np.arange(n_groups), n_labels), normalized.flatten())\n",
    "res = {\n",
    "    \"balanced_accuracy\": {\n",
    "        \"perfect_postprocessed\": (n_groups - postprocessor.score_) / n_groups\n",
    "    },\n",
    "    \"dp_gap_linf_max\": {\n",
    "        \"perfect_predictor\": np.max(diff)\n",
    "    },\n",
    "    \"dp_gap_l1_max\": {\n",
    "        \"perfect_predictor\": np.max(1 / 2 * np.sum(diff, axis=2))\n",
    "    },\n",
    "    \"dp_gap_l1_avg\": {\n",
    "        \"perfect_predictor\":\n",
    "            np.mean(1 / 2 * np.sum(diff, axis=2)[np.triu_indices(n_groups, 1)])\n",
    "    },\n",
    "}\n",
    "\n",
    "display(pd.DataFrame(res))\n",
    "display(grouped / counts)\n",
    "display(pd.DataFrame(counts, columns=[\"Count\"]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize BiasBios dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3401e86286994b5f9e981edbd5313c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/256 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1aca26b98543cc845ea6b152838594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/99 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fc78d5109d4363b37d109f78804b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "  tokenized_examples = tokenizer(examples[\"bio\"],\n",
    "                                 padding=False,\n",
    "                                 max_length=tokenizer.model_max_length,\n",
    "                                 truncation=True)\n",
    "  tokenized_examples[\"labels\"] = examples[\"title\"]\n",
    "  tokenized_examples[\"group_labels\"] = examples[\"gender\"]\n",
    "  return tokenized_examples\n",
    "\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset[\"train\"].column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data split\n",
    "split_dataset = tokenized_dataset[\"train\"].train_test_split(\n",
    "    test_size=split_ratio_for_postprocessing, seed=seed)\n",
    "train_dataset_predictor = split_dataset[\"train\"]\n",
    "train_dataset_postprocessor = split_dataset[\"test\"]\n",
    "eval_dataset = tokenized_dataset[\"test\"]\n",
    "dev_dataset = tokenized_dataset[\"dev\"]\n",
    "\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader_predictor = torch.utils.data.DataLoader(\n",
    "    train_dataset_predictor,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size)\n",
    "train_dataloader_postprocessor = torch.utils.data.DataLoader(\n",
    "    train_dataset_postprocessor,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset,\n",
    "                                              collate_fn=data_collator,\n",
    "                                              batch_size=batch_size)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset,\n",
    "                                             collate_fn=data_collator,\n",
    "                                             batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/train NLU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "is_finetuned = False\n",
    "if os.path.exists(model_dir):\n",
    "  model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_dir).to(device)\n",
    "  is_finetuned = True\n",
    "else:\n",
    "  model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_name, num_labels=n_labels).to(device)\n",
    "\n",
    "model_input_args = list(model.forward.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_finetuned:\n",
    "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "  optimizer_grouped_parameters = [\n",
    "      {\n",
    "          \"params\": [\n",
    "              p for n, p in model.named_parameters()\n",
    "              if not any(nd in n for nd in no_decay)\n",
    "          ],\n",
    "          \"weight_decay\": weight_decay,\n",
    "      },\n",
    "      {\n",
    "          \"params\": [\n",
    "              p for n, p in model.named_parameters()\n",
    "              if any(nd in n for nd in no_decay)\n",
    "          ],\n",
    "          \"weight_decay\": 0.0\n",
    "      },\n",
    "  ]\n",
    "  optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "  lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=(warmup_ratio * n_epochs *\n",
    "                        len(train_dataloader_predictor)),\n",
    "      num_training_steps=n_epochs * len(train_dataloader_predictor))\n",
    "\n",
    "  loss_fn = torch.nn.MSELoss()\n",
    "  # loss_fn = torch.nn.CrossEntropyLoss()\n",
    "  metric = datasets.load_metric(\"accuracy\")\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm.tqdm(train_dataloader_predictor,\n",
    "                           desc=f\"Train epoch {epoch}\"):\n",
    "      batch = {\n",
    "          k: v.to(device) for k, v in batch.items() if k in model_input_args\n",
    "      }\n",
    "      outputs = model(**batch)\n",
    "      labels_one_hot = torch.nn.functional.one_hot(batch[\"labels\"], n_labels)\n",
    "      loss = loss_fn(outputs.logits, labels_one_hot.float())\n",
    "      # loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm.tqdm(dev_dataloader, desc=f\"Dev epoch {epoch}\"):\n",
    "      batch = {\n",
    "          k: v.to(device) for k, v in batch.items() if k in model_input_args\n",
    "      }\n",
    "      with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        probas = outputs.logits\n",
    "        predictions = torch.argmax(probas, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    print(f\"epoch {epoch}: {dumps(metric.compute(), indent=2)}\")\n",
    "\n",
    "  model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe13668ccd3346e28b683df8edd30ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183966dbfbf641389bb84eb23d917d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/3074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>dp_gap_linf_max</th>\n",
       "      <th>dp_gap_l1_max</th>\n",
       "      <th>dp_gap_l1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predictor</th>\n",
       "      <td>0.857643</td>\n",
       "      <td>0.858601</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.286086</td>\n",
       "      <td>0.286086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postprocessor</th>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.806800</td>\n",
       "      <td>0.114265</td>\n",
       "      <td>0.135774</td>\n",
       "      <td>0.135774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  balanced_accuracy  dp_gap_linf_max  dp_gap_l1_max  \\\n",
       "predictor      0.857643           0.858601         0.105691       0.286086   \n",
       "postprocessor  0.805031           0.806800         0.114265       0.135774   \n",
       "\n",
       "               dp_gap_l1_avg  \n",
       "predictor           0.286086  \n",
       "postprocessor       0.135774  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_fn(dataloader):\n",
    "  model.eval()\n",
    "  probas = []\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"Inference\"):\n",
    "      batch = {\n",
    "          k: v.to(device) for k, v in batch.items() if k in model_input_args\n",
    "      }\n",
    "      outputs = model(**batch)\n",
    "      probas.append(projection_simplex(outputs.logits.cpu().numpy(), axis=1))\n",
    "      # probas.append(\n",
    "      #     torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "  return np.concatenate(probas, axis=0)\n",
    "\n",
    "\n",
    "postprocessor_path = os.path.join(model_dir, \"postprocessor.pkl\")\n",
    "if os.path.exists(postprocessor_path):\n",
    "  with open(postprocessor_path, \"rb\") as f:\n",
    "    postprocessor = pickle.load(f)\n",
    "else:\n",
    "  postprocessor = postprocess.postprocess(\n",
    "      predict_fn, train_dataloader_postprocessor,\n",
    "      np.array(train_dataset_postprocessor[\"group_labels\"]))\n",
    "  with open(postprocessor_path, \"wb\") as f:\n",
    "    pickle.dump(postprocessor, f)\n",
    "\n",
    "res = postprocess.evaluate(predict_fn, postprocessor, eval_dataloader,\n",
    "                           np.array(eval_dataset[\"labels\"]),\n",
    "                           np.array(eval_dataset[\"group_labels\"]), n_labels,\n",
    "                           n_groups)\n",
    "display(pd.DataFrame(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165e033eca904a2c862fd1552b9a52c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5aa06f975e4ff5a2d643510cd8f3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/3074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Laplace smoothing:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>dp_gap_linf_max</th>\n",
       "      <th>dp_gap_l1_max</th>\n",
       "      <th>dp_gap_l1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predictor</th>\n",
       "      <td>0.857591</td>\n",
       "      <td>0.858546</td>\n",
       "      <td>0.105732</td>\n",
       "      <td>0.286233</td>\n",
       "      <td>0.286233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postprocessor</th>\n",
       "      <td>0.804342</td>\n",
       "      <td>0.806118</td>\n",
       "      <td>0.114086</td>\n",
       "      <td>0.128004</td>\n",
       "      <td>0.128004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  balanced_accuracy  dp_gap_linf_max  dp_gap_l1_max  \\\n",
       "predictor      0.857591           0.858546         0.105732       0.286233   \n",
       "postprocessor  0.804342           0.806118         0.114086       0.128004   \n",
       "\n",
       "               dp_gap_l1_avg  \n",
       "predictor           0.286233  \n",
       "postprocessor       0.128004  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "postprocessor_path = os.path.join(model_dir, \"postprocessor_smoothing.pkl\")\n",
    "if os.path.exists(postprocessor_path):\n",
    "  with open(postprocessor_path, \"rb\") as f:\n",
    "    postprocessor = pickle.load(f)\n",
    "else:\n",
    "  postprocessor = postprocess.postprocess(\n",
    "      predict_fn,\n",
    "      train_dataloader_postprocessor,\n",
    "      np.array(train_dataset_postprocessor[\"group_labels\"]),\n",
    "      noise_fn=noise_fn,\n",
    "      n_perturbations=10)\n",
    "  with open(postprocessor_path, \"wb\") as f:\n",
    "    pickle.dump(postprocessor, f)\n",
    "\n",
    "res = postprocess.evaluate(predict_fn,\n",
    "                           postprocessor,\n",
    "                           eval_dataloader,\n",
    "                           np.array(eval_dataset[\"labels\"]),\n",
    "                           np.array(eval_dataset[\"group_labels\"]),\n",
    "                           n_labels,\n",
    "                           n_groups,\n",
    "                           noise_fn=noise_fn,\n",
    "                           n_perturbations=1000)\n",
    "print(\"With Laplace smoothing:\")\n",
    "display(pd.DataFrame(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "19e61f44fac9031dd94097741e1a6b3cb28108cf6746ab61e71bd478567bc8e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
